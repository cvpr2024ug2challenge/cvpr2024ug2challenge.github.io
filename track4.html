<!DOCTYPE html>
<html lang="en-US" dir="ltr">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!--  
    Document Title
    =============================================
  -->
  <title>UG2+ Challenge</title>
    <!--  
    Favicons
    =============================================
  -->
  <link rel="apple-touch-icon" sizes="57x57" href="assets/images/favicons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="assets/images/favicons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="assets/images/favicons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="assets/images/favicons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="assets/images/favicons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="assets/images/favicons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="assets/images/favicons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="assets/images/favicons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="assets/images/favicons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192" href="assets/images/favicons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="assets/images/favicons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="assets/images/favicons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="assets/images/favicons/favicon-16x16.png">
  <link rel="manifest" href="/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="assets/images/favicons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
    <!--  
    Stylesheets
    =============================================
    
  -->
  <!-- Default stylesheets-->
  <link href="assets/lib/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet">
  <!-- Template specific stylesheets-->
  <link href="https://fonts.googleapis.com/css?family=Roboto+Condensed:400,700" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Volkhov:400i" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700,800" rel="stylesheet">
  <link href="assets/lib/animate.css/animate.css" rel="stylesheet">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
  <link href="assets/lib/et-line-font/et-line-font.css" rel="stylesheet">
  <link href="assets/lib/flexslider/flexslider.css" rel="stylesheet">
  <link href="assets/lib/owl.carousel/dist/assets/owl.carousel.min.css" rel="stylesheet">
  <link href="assets/lib/owl.carousel/dist/assets/owl.theme.default.min.css" rel="stylesheet">
  <link href="assets/lib/magnific-popup/dist/magnific-popup.css" rel="stylesheet">
  <link href="assets/lib/simple-text-rotator/simpletextrotator.css" rel="stylesheet">
  <!-- Main stylesheet and color file-->
  <link href="assets/css/style.css" rel="stylesheet">
  <link id="color-scheme" href="assets/css/colors/default.css" rel="stylesheet">
</head>
<style>
* {
  box-sizing: border-box;
}

.column {
  float: left;
  width: 24.0%;
  padding: 5px;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>
<body data-spy="scroll" data-target=".onpage-navigation" data-offset="60">
  <a id="ddmenuLink" href="menu_transparent.html">Menu</a>
  <main>
    <div class="page-loader">
      <div class="loader">Loading...</div>
    </div>


    <div class="main">
      <section class="module bg-dark-30 portfolio-page-header" data-background="assets/images/t2bg.jpg" style="padding: 30px 0;">
        <div class="container" style="width:100%">
          <div class="row" style="padding-top: 40px">
            <div class="col-sm-6 col-sm-offset-3">
              <!-- <p style="float: right;"><img height="300" width="300" src="pics/smoke_generator.png"></p> -->
              <h2 class="module-title font-alt" style="margin: 0 0 0px">Track 4: 3D reconstruction from low light/smoky videos</h2>
              <h3 class="module-subtitle font-serif" style="margin: 0 0 20px"><a href="https://docs.google.com/forms/d/e/1FAIpQLSepaa-sOJRAgwCG7V-ubR-lmuJKZx6gCj-YftFLCDAmc3CmEA/viewform?usp=sf_link" target="_blank" class="section-scroll btn btn-border-w btn-round">Register for this track</a></h3>
            </div>
          </div>
        </div>
      </section>


      <section class="module-medium" style="padding-bottom: 0px"></section>

      <!-- GT-RAIN Dataset Figure -->
      <div class="container" style="padding-bottom: 20px">
        <figure>
          <img src="assets/images/smoky_temp.PNG" style="width:75%;height:75%;margin-left:auto;margin-right:auto;display:block;border: none;">
          <figcaption style="width:80%;margin-left:auto;margin-right:auto;display:block;text-align: center;">
            <em>[Left] Visualization of the computer-aided design (CAD) model which we realize with Lego blocks. [Right two images] The realization of the CAD model viewed in the presence of artificial fog via UAV.. </em></figcaption>
        </figure>
    </div>


      
        <div class="container">
          <div class="row" style="text-align: justify">
            <div class="col-sm-12 font-nat" style="font-size: 13pt">
              <!--
              <p>Images captured in adverse weather conditions significantly impact the performance of many vision tasks. Rain is a common weather phenomenon that introduces visual degradations to captured images and videos through partial occlusions of objects – in heavy rain, severe occlusion to the background. As most vision algorithms assume clear weather, with no interference of rain, their performance suffers. Deraining is the task of removing such visual degradations so that the images are better suited to the assumptions of downstream vision algorithms, as well as for aesthetic fruition. </p>

              <p>UG<sup>2</sup>+ Track 3 aims to promote the development of novel single image deraining algorithms for real images. The competition will feature diverse and challenging scenarios that include (i) various types of rain conditions (i.e. long and short streaks, various densities and accumulation, with and without rain fog), (ii) large variety of background scenes from urban locations (i.e. buildings, streets, cityscapes) to natural scenery (i.e. forests, plains, hills), (iii) varying degrees of illumination from different times of day, and all of which are captured by (iv) cameras that cover a wide array of resolutions, noise levels, and intrinsic parameters. In a collaboration with the authors of GT-RAIN, we will introduce an additional 15 extra scenes set aside as a benchmark test set. The challenge will be split into three phases (training, validation, and testing), where data corresponding to each phase will be released to the participants. The validation set will consist of 5400 frames covering 18 scenes and the testing set 4500 frames covering 15 scenes. Unlike previous evaluation protocols that were limited to qualitative evaluation on real images, the submissions will be evaluated quantitatively using standard metrics like PSNR and SSIM. The ranking of algorithms will be determined based on evaluation scores on the testing set. The first place winner will be awarded $1000 USD, second place will be awarded $800 USD, and third place $500 USD. </p>
              -->

              <p>Computer vision in low visibility is a problem that has yet to be solved. The technology of the current world includes night vision cameras implemented by Apple and Samsung in a competition to have the best phone camera with every iterative model. However, this problem does not only manifest itself in today’s handheld devices. Very few datasets exist incorporating low visibility environments. It is important to expand the limits of what computer vision can do. Since we don’t have knowledge in computer vision when there are dust storms, thick fog, extremely bright ambient lighting, or extremely dark ambient lighting, it is crucial we create datasets in these environments and develop algorithms so that computer vision can be broadened to handle any sort of environment. 
</p>

              <p>You may ask what the applications of these solution algorithms would be. Parties who would benefit range from military personnel who often work in adverse conditions to your parents driving through an unknown area in which they are unable to see their immediate surroundings. In keeping your parents safe and the military efficient, enhancing computer vision systems to “see” for them in adverse environments brought by the onset of unexpected fog and/or blinding rain, and extremely bright/dark conditions is imminent. Mars rovers and satellites also have potential to benefit from these enhanced computer vision systems as they often operate in adverse and unknown environments where we ourselves are unable to physically solve their vision issues in real time. 
</p>

              <p>Currently, we have a vast collection of great datasets in a satisfactory environment. We have looked at a set examining traffic in low altitudes..We have used this as inspiration to create our control set. In terms of low lighting, we have come across one particular study where a dataset has been generated of pedestrians walking at night. This collection was created using infrared cameras to enhance object detection of people, vehicles and other moving objects. In terms of haze, we have found another study detailing the creation of a dataset using Unmanned Ground Vehicles and Unmanned Aerial Vehicles. This set has both a control in clear environments as well as a degraded set using professional fog machines. As we continue to develop our solution, we will look into more previous articles for inspiration and knowledge to build a novel solution.
</p>
              <p>So what are we doing as a team? Our aim is to create a dataset of drone footage flying through several original mini cities with varying visibility dependent on lighting and fog. Currently, we have a control dataset over two mini cities. It is up to us to design the most complex mini city so that the 3D reconstruction algorithms are challenged. Once the mini cities are constructed in the lab, the team will fly the drone in varying lighting and fog settings. Approximately one week will be given per map. Video footage will be gathered and analyzed in the duration of the process to check for quality of images in terms of resolution, altitude, map coverage and angles. Metrics will be subject to change as more is learned from background research and how the project goes in reality. 
</p>
              <p>Ultimately, our objective is to develop a sample solution that will reconstruct these mini cities with high resolution and clarity despite any adverse environmental factors they are in. Join us as we continue to develop our dataset and find various algorithms to enhance computer vision in low visibility environments. 
</p>

              <p>Submission criteria of this challenge require all participants to submit a fact sheet detailing their method, datasets used, runtime etc. The fact sheets will be compiled into a final report post challenge to highlight trends and innovative techniques. Participants are encouraged to submit manuscripts detailing their method to the workshop.</p>
                <!-- <ol>
                  <li>(Semi-)Supervised Object Detection in Haze Conditions</li>
                </ol> -->
              
              <!-- <p style="text-align: justify; ">
                <!-- In all two sub-challenges, the participant teams are allowed to use external training data that are not mentioned above, including self-synthesized or self-collected data; -->
                <!-- <b>but they must state so in their submissions ("Method description" section in Codalab)</b>. -->
                <!-- Each leaderboard will be divided into two ranking lists: with and without external data. -->
                <!-- The ranking criteria will be the Mean average precision (mAP) on each testing set, with Interception-of-Union (IoU) threshold as 0.5. -->
                <!-- If the ratio of the intersection of a detected region with an annotated face region is greater than 0.5, a score of 1 is assigned to the detected region, and 0 otherwise. -->
                <!-- When mAPs with IoU as 0.5 are equal, the mAPs with higher IoUs (0.6, 0.7, 0.8) will be compared sequentially. -->
              <!-- </p> -->
              	<!-- <ul> -->


            </div>
          </div>

        </div>
      </section>
      <hr class="divider-w">

    <a id="ddfooterLink" href="footer.html">Footer</a>
      <div class="scroll-up"><a href="#totop"><i class="fa fa-angle-double-up"></i></a></div>
  </main>
    <!--  
    JavaScripts
    =============================================
    -->
    <script src="assets/lib/jquery/dist/jquery.js"></script>
    <script src="assets/lib/bootstrap/dist/js/bootstrap.min.js"></script>
    <script src="assets/lib/wow/dist/wow.js"></script>
    <script src="assets/lib/jquery.mb.ytplayer/dist/jquery.mb.YTPlayer.js"></script>
    <script src="assets/lib/isotope/dist/isotope.pkgd.js"></script>
    <script src="assets/lib/imagesloaded/imagesloaded.pkgd.js"></script>
    <!-- <script src="assets/lib/flexslider/jquery.flexslider.js"></script> -->
    <script src="assets/lib/owl.carousel/dist/owl.carousel.min.js"></script>
    <!-- <script src="assets/lib/smoothscroll.js"></script> -->
    <script src="assets/lib/magnific-popup/dist/jquery.magnific-popup.js"></script>
    <script src="assets/lib/simple-text-rotator/jquery.simple-text-rotator.min.js"></script>
    <script src="assets/js/plugins.js"></script>
    <script src="assets/js/main.js"></script>
    <script src="assets/js/ddmenu.js" type="text/javascript"></script>
    <script src="assets/js/ddfooter.js" type="text/javascript"></script>
</body>
</html>
